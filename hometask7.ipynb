{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4,  8, 19,  4, 14,  6,  0,  1,\n",
       "        7, 12,  5,  0, 10,  6,  2,  4,  1, 12,  9, 15,  7,  6, 13, 12, 17,\n",
       "       18, 10,  8, 11,  8, 16,  9,  4,  3,  9,  9,  4,  4,  8, 12, 14,  5,\n",
       "       15,  2, 13, 17, 11,  7, 10,  2, 14, 12,  5,  4,  6,  7,  0, 11, 16,\n",
       "        0,  6, 17,  7, 12,  7,  3, 12, 11,  7,  2,  2,  0, 16,  1,  2,  7,\n",
       "        3,  2,  1, 10, 12, 12, 17, 12,  2,  8,  8, 18,  5,  0,  1])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(binary=True, max_df=0.04, min_df=10,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, min_df=10, max_df=.04)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I was wondering if anyone out there could enlighten me on this car I saw'\n",
    "x = vectorizer.transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0]), array([ 1905,  3576,  8221, 10138]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['car', 'enlighten', 'saw', 'wondering'], dtype='<U79')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 10299)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета, сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10138)\t1\n",
      "  (0, 3576)\t1\n",
      "  (0, 1905)\t1\n",
      "  (0, 8221)\t1\n",
      "  (0, 3288)\t1\n",
      "  (0, 8787)\t1\n",
      "  (0, 5726)\t1\n",
      "  (0, 5506)\t1\n",
      "  (0, 542)\t1\n",
      "  (0, 3399)\t1\n",
      "  (0, 593)\t1\n",
      "  (0, 3289)\t1\n",
      "  (0, 8633)\t1\n",
      "  (0, 824)\t1\n",
      "  (0, 1791)\t1\n",
      "  (0, 8381)\t1\n",
      "  (0, 7936)\t1\n",
      "  (0, 1628)\t1\n",
      "  (0, 6155)\t1\n",
      "  (0, 3560)\t1\n",
      "  (0, 8750)\t1\n",
      "  (0, 7337)\t1\n",
      "  (0, 4668)\t1\n",
      "  (0, 4995)\t1\n",
      "  (1, 3856)\t1\n",
      "  :\t:\n",
      "  (11313, 3560)\t1\n",
      "  (11313, 4461)\t1\n",
      "  (11313, 9590)\t1\n",
      "  (11313, 1858)\t1\n",
      "  (11313, 4708)\t1\n",
      "  (11313, 8405)\t1\n",
      "  (11313, 8393)\t1\n",
      "  (11313, 10065)\t1\n",
      "  (11313, 9464)\t1\n",
      "  (11313, 1610)\t1\n",
      "  (11313, 8530)\t1\n",
      "  (11313, 8907)\t1\n",
      "  (11313, 6844)\t1\n",
      "  (11313, 1387)\t1\n",
      "  (11313, 6118)\t1\n",
      "  (11313, 7091)\t1\n",
      "  (11313, 9192)\t1\n",
      "  (11313, 8797)\t1\n",
      "  (11313, 6111)\t1\n",
      "  (11313, 1963)\t1\n",
      "  (11313, 7062)\t1\n",
      "  (11313, 8010)\t1\n",
      "  (11313, 9414)\t1\n",
      "  (11313, 5647)\t1\n",
      "  (11313, 5392)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-117-1d30359d2693>:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(niter)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeebf003c9d428aac68967b55e3f130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tag 0: ['algorithm' 'chip' 'clipper' 'encryption' 'keys' 'phone' 'public'\n",
      " 'secure' 'security' 'soon']\n",
      "Tag 1: ['1993' '93' 'ca' 'computer' 'contact' 'date' 'ftp' 'net' 'posted'\n",
      " 'university']\n",
      "Tag 2: ['btw' 'couple' 'guy' 'mark' 'mentioned' 'mike' 'net' 'posting' 'sorry'\n",
      " 'thank']\n",
      "Tag 3: ['agree' 'argument' 'certainly' 'discussion' 'evidence' 'making' 'saying'\n",
      " 'sense' 'simply' 'understand']\n",
      "Tag 4: ['00' '100' 'asking' 'condition' 'includes' 'offer' 'original' 'price'\n",
      " 'sell' 'shipping']\n",
      "Tag 5: ['27' '34' 'ah' 'hi' 'hp' 'id' 'ma' 'mi' 'mr' 'ms']\n",
      "Tag 6: ['center' 'cost' 'development' 'earth' 'launch' 'low' 'nasa' 'research'\n",
      " 'systems' 'technology']\n",
      "Tag 7: ['bible' 'christ' 'christian' 'christians' 'church' 'faith' 'love' 'man'\n",
      " 'religion' 'says']\n",
      "Tag 8: ['american' 'country' 'crime' 'federal' 'guns' 'law' 'laws' 'national'\n",
      " 'rights' 'states']\n",
      "Tag 9: ['address' 'deleted' 'guess' 'haven' 'hey' 'news' 'sorry' 'stuff' 'wonder'\n",
      " 'yeah']\n",
      "Tag 10: ['disease' 'doctor' 'food' 'interesting' 'medical' 'normal' 'results'\n",
      " 'taking' 'treatment' 'usually']\n",
      "Tag 11: ['11' 'games' 'hockey' 'league' 'play' 'players' 'season' 'team' 'teams'\n",
      " 'win']\n",
      "Tag 12: ['bike' 'cars' 'check' 'engine' 'fast' 'miles' 'road' 'small' 'speed'\n",
      " 'turn']\n",
      "Tag 13: ['advance' 'comes' 'couldn' 'david' 'especially' 'knows' 'mind' 'opinions'\n",
      " 'steve' 'wondering']\n",
      "Tag 14: ['deal' 'gets' 'hear' 'light' 'media' 'ok' 'somebody' 'sorry' 'sort'\n",
      " 'sounds']\n",
      "Tag 15: ['application' 'code' 'display' 'files' 'hi' 'running' 'server' 'version'\n",
      " 'window' 'write']\n",
      "Tag 16: ['days' 'happened' 'home' 'left' 'night' 'saw' 'started' 'told' 'took'\n",
      " 'wanted']\n",
      "Tag 17: ['arab' 'children' 'history' 'israel' 'israeli' 'jewish' 'jews' 'killed'\n",
      " 'population' 'today']\n",
      "Tag 18: ['circuit' 'company' 'dave' 'electronics' 'folks' 'internet' 'office'\n",
      " 'phone' 'university' 'voltage']\n",
      "Tag 19: ['board' 'computer' 'disk' 'mac' 'memory' 'monitor' 'pc' 'ram' 'speed'\n",
      " 'video']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def LDA(X, niter = 100, alpha = np.ones(20)):\n",
    "    K = 20\n",
    "    N = X.shape[1]\n",
    "    Betta = np.ones(N)\n",
    "    Docs = X.nonzero()[0]#номера документов для слов из словаря \n",
    "    Words = X.nonzero()[1]#место в словаре\n",
    "    n_KW =  np.zeros((K, N))#число вхождений слова в тег из словаря\n",
    "    n_DK = np.zeros((len(Docs), K))#число слов тега K в документе D\n",
    "    nK = np.zeros(K)\n",
    "    tegs = np.random.choice(K, len(Docs))\n",
    "    for Doc, Word, teg in zip(Docs, Words, tegs):#одновременно проходимся по трем массивам\n",
    "        n_KW[teg, Word] += 1\n",
    "        n_DK[Doc, teg] += 1\n",
    "        nK[teg] += 1\n",
    "    for i in tqdm(range(niter)):\n",
    "        for j in range(len(Docs)):\n",
    "            teg = tegs[j]\n",
    "            Doc = Docs[j]\n",
    "            Word = Words[j]\n",
    "            n_KW[teg, Word] -= 1\n",
    "            n_DK[Doc, teg] -= 1\n",
    "            nK[teg] -= 1\n",
    "            \n",
    "            P = (n_DK[Doc, :] + alpha)*(n_KW[:, Word] + Betta[Word])/(nK + Betta.sum())\n",
    "            P = np.array(P)\n",
    "            P /= P.sum()\n",
    "            tegs[j] = np.random.choice(np.arange(K), p = P)\n",
    "            teg = tegs[j]\n",
    "            n_KW[teg, Word] += 1\n",
    "            n_DK[Doc, teg] += 1\n",
    "            nK[teg] += 1\n",
    "            \n",
    "    T = [np.argsort(n_KW[k, :]) for k in range(K)]\n",
    "    T = np.array(T)\n",
    "    for k in range(K):\n",
    "        a = np.zeros(N)\n",
    "        for j in T[k, -11: -1]:\n",
    "            a[j] = 1\n",
    "        print('Tag {}: {}'.format(k, '\\\\t'.join(str(x) for x in vectorizer.inverse_transform(a))))\n",
    "LDA(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге можно заметить, что слова довольно хорошо разбились по своим темам, которые легко выделить.\n",
    "Например, слова из Tag 0 относятся к криптографии, из Tag 7 - к религии (христианству), из Tag 12 - к гонкам, из Tag 19 - к составляющим компьютера. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
